#+TITLE: Edit videos based on background color and remove dead air

* Background and Goals


* Imports

#+name: imports
#+begin_src python
import math
import sys
import os
from typing import Tuple
from moviepy.editor import AudioClip, VideoFileClip, concatenate_videoclips
from timeit import default_timer as timer
from datetime import timedelta
#+end_src

* Export EDL
Given a list of (start, end) time intervals, export an EDL file.

#+name: export_edl
#+begin_src python
# Given a timestamp in seconds, convert to a string in the format HH:MM:SS:FF
def seconds_to_ts(seconds, fps):
  hours = math.floor(seconds / 3600)
  minutes = math.floor((seconds - (hours * 3600)) / 60)
  secs = math.floor(seconds - (hours * 3600) - (minutes * 60))
  frames = math.floor((seconds - math.floor(seconds)) * fps)
  return '{:02d}:{:02d}:{:02d}:{:02d}'.format(hours, minutes, secs, frames)

# Export an EDL file given a list of (start, end) time intervals.
def export_edl(intervals, clip_filename, edl_filename, fps):
  print('Exporting EDL file: {}'.format(edl_filename))
  with open(edl_filename, 'w') as f:
    f.write('TITLE: Timeline 1\n')
    f.write('FCM: NON-DROP FRAME\n')
    timeline_seconds = 60 * 60  # Davinci timeline starts at 1 hr.
    for i, interval in enumerate(intervals):
      start = interval[0]
      end = interval[1]
      duration = end - start
      timeline_start = timeline_seconds
      timeline_end = timeline_seconds + duration
      timeline_seconds = timeline_end
      f.write('{:03d}  AX       V     C        {} {}  {}  {}\n'.format(
        i + 1,
        seconds_to_ts(start, fps),
        seconds_to_ts(end, fps),
        seconds_to_ts(timeline_start, fps),
        seconds_to_ts(timeline_end, fps)))
      f.write('* FROM CLIP NAME: {}\n'.format(clip_filename))
#+end_src


* Color Edit
** Get average RGB of part of the frame
This is used to determine whether we're seeing a "green" or "red" frame.

#+name: avg_rgb
#+begin_src python
# Get average RGB of part of a frame. Frame is H * W * 3 (rgb)
# Assumes x1 < x2, y1 < y2
def avg_rgb(frame, x1: int, y1: int, x2: int, y2: int) -> Tuple[float, float, float]:
    r, g, b = 0, 0, 0
    for x in range(x1, x2):
        for y in range(y1, y2):
            r += frame[x, y, 0]
            g += frame[x, y, 1]
            b += frame[x, y, 2]
    total_pixels = (x2 - x1) * (y2 - y1)
    avg_r = r / total_pixels
    avg_g = g / total_pixels
    avg_b = b / total_pixels
    return avg_r, avg_g, avg_b
#+end_src

** Edit video based on color

#+name: color_edit_intervals
#+begin_src python :noweb yes
<<avg_rgb>>

# Look for colors in frame, edit based on that.
# Returns list of (start, end) tuples of time intervals we want to keep.
def color_edit_intervals(video):
    intervals_to_keep = []
    frame_marker = [] # 'c': content; 'y': keep prior interval; 'n': drop prior interval.
    # Iterate over every frame.
    for frame in video.iter_frames():
        avg_r, avg_g, avg_b = avg_rgb(frame, 900, 500, 910, 510)
        is_red = (avg_r > 120) and (avg_g < 50) and (avg_b < 50)
        is_green = (avg_r < 80) and (avg_g > 120) and (avg_b < 50)
        marker = 'c'
        if is_red:
            marker = 'n'
        elif is_green:
            marker = 'y'
        frame_marker.append(marker)

    keep_start, keep_end = 0, 0
    keep_intervals = []
    start_of_last_green = 0
    for i in range(1, len(frame_marker)):
        m1 = frame_marker[i - 1]
        m2 = frame_marker[i]
        # Content followed by green, take note.
        if m1 == 'c' and m2 == 'y':
            start_of_last_green = i
        # Green followed by content. Keep previous interval. Start a (possible) new interval.
        if m1 == 'y' and m2 == 'c':
            keep_end = start_of_last_green / video.fps
            keep_intervals.append([keep_start, keep_end])
            keep_start = (i + 1) / video.fps
        # Red followed by content. Drop the previous interval. Start a (possible) new interval.
        if m1 == 'n' and m2 == 'c':
            keep_start = i / video.fps
    
    # Ending on green with no following content.
    last_index = len(frame_marker) - 1
    if frame_marker[last_index] == 'c' or frame_marker[last_index] == 'y':
        keep_end = i / video.fps
        keep_intervals.append([keep_start, keep_end])

    return keep_intervals
#+end_src

** Get back video clip edited with color
The above routine, which does the heavy lifting, returns a list of (start, end) time intervals. This small utility converts that to a MoviePy videoclip.

#+name: color_edit
#+begin_src python :noweb yes
<<color_edit_intervals>>

def color_edit(vid_file_clip):
    print("---- Looking for color coded editing clips... -----")

    start = timer()
    intervals_to_keep = color_edit_intervals(vid_file_clip)
    print("Keeping color edit intervals: " + str(intervals_to_keep))
    keep_clips = [vid_file_clip.subclip(start, end) for [start, end] in intervals_to_keep]
    color_edited_video = concatenate_videoclips(keep_clips)
    end = timer()

    color_edit_time = timedelta(seconds=end-start)
    print('Color edit time: ' + str(color_edit_time))

    return color_edited_video, intervals_to_keep
#+end_src


* Remove dead air
Look for speaking parts, cut out the silent parts.

** Find speaking intervals
#+name: find_speaking_intervals
#+begin_src python
# Iterate over audio to find the non-silent parts. Outputs a list of
# (speaking_start, speaking_end) intervals.
# Args:
#  window_size: (in seconds) hunt for silence in windows of this size
#  volume_threshold: volume below this threshold is considered to be silence
#  ease_in: (in seconds) add this much silence around speaking intervals
def find_speaking_intervals(audio_clip, window_size=0.1, volume_threshold=0.05, ease_in=0.1, audio_fps=44100):
    # First, iterate over audio to find all silent windows.
    num_windows = math.floor(audio_clip.end/window_size)
    window_is_silent = []
    for i in range(num_windows):
        s = audio_clip.subclip(i * window_size, (i + 1) * window_size).set_fps(audio_fps)
        v = s.max_volume()
        window_is_silent.append(v < volume_threshold)

    # Find speaking intervals.
    speaking_start = 0
    speaking_end = 0
    speaking_intervals = []
    for i in range(1, len(window_is_silent)):
        e1 = window_is_silent[i - 1]
        e2 = window_is_silent[i]
        # silence -> speaking
        if e1 and not e2:
            speaking_start = i * window_size
        # speaking -> silence, now have a speaking interval
        if not e1 and e2:
            speaking_end = i * window_size
            new_speaking_interval = [max(0, speaking_start - ease_in), speaking_end + ease_in]
            # With tiny windows, this can sometimes overlap the previous window, so merge.
            need_to_merge = len(speaking_intervals) > 0 and speaking_intervals[-1][1] > new_speaking_interval[0]
            if need_to_merge:
                merged_interval = [speaking_intervals[-1][0], new_speaking_interval[1]]
                speaking_intervals[-1] = merged_interval
            else:
                speaking_intervals.append(new_speaking_interval)

    return speaking_intervals
#+end_src

** Get back video clip without dead air
The above routine did all the heavy lifting, returning a list of (start, end) time intervals. This is a utility to concatenate clips to return one clip without dead air.

#+name: find_speaking
#+begin_src python :noweb yes
<<find_speaking_intervals>>

def find_speaking(input_clip, input_audio_fps):
    print("\n\n\n----- Now cutting out dead air... -----")

    start = timer()
    speaking_intervals = find_speaking_intervals(input_clip.audio, audio_fps=input_audio_fps)
    print("Keeping speaking intervals: " + str(speaking_intervals))
    speaking_clips = [input_clip.subclip(start, end) for [start, end] in speaking_intervals]
    final_video = concatenate_videoclips(speaking_clips)
    end = timer()

    speaking_detection_time = timedelta(seconds=end-start)
    print('Speaking detection time: ' + str(speaking_detection_time))

    return final_video, speaking_intervals
#+end_src

* Main
#+name: main
#+begin_src python
def main():
    # Parse args
    # Input file path
    file_in = sys.argv[1]
    # Output file path
    file_out = sys.argv[2]

    vid = VideoFileClip(file_in)

    # Color edit.
    color_edited_video, color_intervals = color_edit(vid)

    # Cut out dead air.
    no_dead_air_video, speaking_intervals = find_speaking(color_edited_video, vid.audio.fps)

    # Write out EDL files with intervals.
    clip_name = os.path.split(file_in)[-1]
    clip_dir = os.path.dirname(file_in)
    color_edl = os.path.join(clip_dir, clip_name + '.color.edl')
    speaking_edl = os.path.join(clip_dir, clip_name + '.speaking.edl')
    export_edl(color_intervals, clip_name, color_edl, fps=vid.fps)
    # The below will not work, because it should be emitting timestamps relative
    # to the color-edited video, but doesn't.
    # export_edl(speaking_intervals, clip_name, speaking_edl)


    print("\n\n\n----- Writing out edited video... -----")
    start = timer()
    no_dead_air_video.write_videofile(file_out,
        #fps=60,
        preset='ultrafast',
        codec='libx264',
        #codec='h264_videotoolbox',
        temp_audiofile='temp-audio.m4a',
        remove_temp=True,
        audio_codec="aac",
        #threads=6,
        ffmpeg_params = ['-threads', '8'],
    )
    vid.close()
    end=timer()

    render_time = timedelta(seconds=end-start)
    print('Render time: ' + str(render_time))


if __name__ == '__main__':
    main()
#+end_src


* Final Assembly
Put together all the above pieces, tangle them into a single Python source file that we can run.

#+name: full_program
#+begin_src python :tangle yes :noweb yes :shebang "#!/usr/bin/env python3"
<<imports>>
<<export_edl>>
<<color_edit>>
<<find_speaking>>

<<main>>
#+end_src
